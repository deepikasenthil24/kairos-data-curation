{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a899477c-68ad-4781-adc3-15b1514c51ce",
   "metadata": {},
   "source": [
    "# KAIROS valuation of iNaturalist subset using clean insect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70e9a2-f987-4669-808c-f3defa2a94b5",
   "metadata": {},
   "source": [
    "https://github.com/lodino/kairos/blob/main/examples/image-data.ipynb \n",
    "\n",
    "#### TODO\n",
    "- Currently just transferred code from Q1 experiments on adult income dataset. Will adapt for the clean insect data (validation) and iNaturalist dataset (training/messy).\n",
    "- Make output the top X highest valued images from the inaturalist dataset (only the insects as theoretically labeled by humans) to use for finetuning ResNet-50.\n",
    "\n",
    "Collecting \"noisy\" indexes: if label is not one of the species in our main insect categories, it is noisy. We might create a semi-noisy label for images that are insects but are just not ones we are looking for. Since the method could value them as \"clean\" and it wouldn't be totally wrong, we just want to know how accurate Kairos is being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd8e70-3afd-4e34-ba4d-1259139382dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from custom_valuations import *\n",
    "from utils import *\n",
    "\n",
    "import opendataval\n",
    "from opendataval.experiment import ExperimentMediator\n",
    "from opendataval.dataval.api import DataEvaluator, ModelLessMixin\n",
    "from opendataval.dataval import DataOob, LavaEvaluator, DVRL\n",
    "from opendataval.experiment import discover_corrupted_sample, noisy_detection\n",
    "from opendataval.dataloader import Register, DataFetcher, mix_labels, add_gauss_noise\n",
    "from opendataval.model import ClassifierMLP, LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from custom_valuations import *\n",
    "from utils import *\n",
    "\n",
    "import custom_valuations\n",
    "import utils\n",
    "import fixed_valuations\n",
    "import importlib\n",
    "importlib.reload(custom_valuations)\n",
    "importlib.reload(utils)\n",
    "importlib.reload(fixed_valuations)\n",
    "from custom_valuations import *\n",
    "from utils import *\n",
    "from fixed_valuations import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "PATH_TO_DATA=\"data_files\" #change based on working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8979d-1ed6-46e2-a51c-0e0a0e622edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    'DataOob': 'o',\n",
    "    'KNNShapley': 's',\n",
    "    'FixedKNNShapley': 's',\n",
    "    'FixedLavaEvaluator': 'x',\n",
    "    #'LavaEvaluator': 'x',\n",
    "    'DVRL': 'x',\n",
    "    'Kairos': '^',\n",
    "}\n",
    "\n",
    "def write_dict(d, fname):\n",
    "    txt = json.dumps(d)\n",
    "    with open(f'logs/{fname}.json', 'w+') as f:\n",
    "        f.write(txt)\n",
    "        \n",
    "train_count, valid_count, test_count = ...,...,...\n",
    "train_kwargs = {\"epochs\": 3, \"batch_size\": 100, \"lr\": 0.01}\n",
    "metric_name = ... #'accuracy'\n",
    "\n",
    "fetcher = (\n",
    "    DataFetcher('inat-embeddings', '../data_files/', \n",
    "                False, random_state=42)\n",
    "    .split_dataset_by_count(train_count,\n",
    "                            valid_count,\n",
    "                            test_count)  \n",
    ")\n",
    "\n",
    "### 1/22/26 check why labels aren't separated by clean vs noisy. How are they aligned?\n",
    "clean_embeddings = np.load(\"../data_files/cifar10-embeddings/clean_embeddings.npy\")\n",
    "labels = np.load(\"../data_files/cifar10-embeddings/labels.npy\")\n",
    "noisy_embeddings = np.load(\"../data_files/cifar10-embeddings/noisy_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2225a-7c6c-4db6-b398-0a4ef79a05f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_presplit_dataset(dataset_name, exp_num, data_frac, path_to_data, cache_dir=None, force_download=False):\n",
    "        # Load CSVs into numpy arrays\n",
    "        x_train = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/X_train_dirty.csv\").values\n",
    "        y_train = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/y_train_dirty.csv\").values\n",
    "\n",
    "        x_valid = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/X_val.csv\").values\n",
    "        y_valid = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/y_val.csv\").values\n",
    "\n",
    "        ### not sure if test is needed. either valid or test.\n",
    "    \n",
    "        # x_test = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/X_test.csv\").values\n",
    "        # y_test = pd.read_csv(f\"{path_to_data}/{dataset_name}/experiment{exp_num}/y_test.csv\").values\n",
    "\n",
    "        # Flatten labels to shape (N,)\n",
    "        y_train = y_train.ravel().astype(int)\n",
    "        y_valid = y_valid.ravel().astype(int)\n",
    "        #y_test  = y_test.ravel().astype(int)\n",
    "\n",
    "        # Take subsets\n",
    "        n_train = int(len(x_train) * data_frac)\n",
    "        n_valid = int(len(x_valid) * data_frac)\n",
    "        #n_test  = int(len(x_test) * data_frac)\n",
    "\n",
    "        x_train = x_train[:n_train]\n",
    "        y_train = y_train[:n_train]\n",
    "\n",
    "        x_valid = x_valid[:n_valid]\n",
    "        y_valid = y_valid[:n_valid]\n",
    "\n",
    "        # x_test = x_test[:n_test]\n",
    "        # y_test = y_test[:n_test]\n",
    "\n",
    "        num_classes = int(max(y_train.max(), y_valid.max())) + 1 #, y_test.max()\n",
    "\n",
    "        # One-hot encode\n",
    "        y_train = np.eye(num_classes)[y_train]\n",
    "        y_valid = np.eye(num_classes)[y_valid]\n",
    "        #y_test  = np.eye(num_classes)[y_test]\n",
    "\n",
    "        covariates = (x_train, x_valid) #, x_test\n",
    "        labels = (y_train, y_valid) #, y_test\n",
    "\n",
    "        # print(f\"num_classes: {num_classes}\")\n",
    "        # print(f\"y_train min/max: {y_train.min()}/{y_train.max()}\")\n",
    "        # print(f\"y_train shape: {y_train.shape}\")\n",
    "        # print(f\"Any labels >= num_classes? {(y_train >= num_classes).any()}\")\n",
    "        # print(f\"Any NaN in y_train? {np.isnan(y_train).any()}\")\n",
    "        # print(f\"Unique labels: {np.unique(y_train)}\")\n",
    "\n",
    "        return covariates, labels\n",
    "\n",
    "### plot accuracy\n",
    "def plot_valuations(eval_name, eval_med, fetcher, dataset_name, exp_num):\n",
    "\n",
    "        evalu = None\n",
    "        for evaluator in eval_med.data_evaluators:\n",
    "                if evaluator.__class__.__name__ == eval_name:\n",
    "                        evalu = evaluator\n",
    "                        break\n",
    "\n",
    "        if evalu is None:\n",
    "                raise RuntimeError(f\"{eval_name} evaluator not found in eval_med\")\n",
    "\n",
    "        valuations = evalu.data_values  # length = n_train\n",
    "\n",
    "        # Identify noisy vs clean indices\n",
    "        noisy = np.array(fetcher.noisy_train_indices)\n",
    "        clean = np.setdiff1d(np.arange(len(valuations)), noisy)\n",
    "\n",
    "        vals_noisy = valuations[noisy]\n",
    "        vals_clean = valuations[clean]\n",
    "        print(len(vals_clean), len(vals_noisy))\n",
    "\n",
    "        plt.figure(figsize=(6,5))\n",
    "\n",
    "        # Option A: Histogram curves (what your example image uses)\n",
    "        plt.hist(vals_clean, bins=50, density=True, histtype='step', linewidth=2, label=\"Clean\", color='blue')\n",
    "        plt.hist(vals_noisy, bins=50, density=True, histtype='step', linewidth=2, label=\"Noisy\", color='red')\n",
    "\n",
    "        # Option B: KDE smooth curves (optional)\n",
    "        # from scipy.stats import gaussian_kde\n",
    "        # xs = np.linspace(min(valuations), max(valuations), 500)\n",
    "        # plt.plot(xs, gaussian_kde(vals_clean)(xs), label=\"Clean KDE\", color='blue')\n",
    "        # plt.plot(xs, gaussian_kde(vals_noisy)(xs), label=\"Noisy KDE\", color='red')\n",
    "\n",
    "        plt.xlabel(f\"{eval_name} Valuation Score\")\n",
    "        plt.ylabel(\"Frequency (Density)\")\n",
    "        plt.title(f\"{eval_name} {dataset_name} Valuations: Clean vs Noisy (Exp {exp_num})\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def thresh_acc(eval_name, eval_med, dataset_name, exp_num, fetcher):\n",
    "        evalu = None\n",
    "        for evaluator in eval_med.data_evaluators:\n",
    "            if evaluator.__class__.__name__ == eval_name:\n",
    "                evalu = evaluator\n",
    "                break\n",
    "\n",
    "        if evalu is None:\n",
    "            raise RuntimeError(f\"{eval_name} evaluator not found in eval_med\")\n",
    "\n",
    "        valuations = evalu.data_values\n",
    "        \n",
    "        vals = valuations.reshape(-1,1)\n",
    "        gmm = GaussianMixture(n_components=2, random_state=42).fit(vals)\n",
    "\n",
    "        # responsibilities: P(component k | v_i)\n",
    "        probs = gmm.predict_proba(vals)\n",
    "\n",
    "        # Determine which component = noisy (lower mean)\n",
    "        means = gmm.means_.reshape(-1)\n",
    "        noisy_component = np.argmin(means)  \n",
    "\n",
    "        # threshold = decision boundary between gaussians\n",
    "        threshold = np.mean([\n",
    "            means[noisy_component],\n",
    "            means[1-noisy_component]\n",
    "        ])\n",
    "        \n",
    "        print(f\"{eval_name} {dataset_name} experiment {exp_num}\")\n",
    "        print(\"GMM threshold:\", round(threshold, 4))\n",
    "\n",
    "        pred = (valuations < threshold).astype(int)  \n",
    "\n",
    "        y_true = np.zeros(len(valuations), dtype=int)\n",
    "        y_true[fetcher.noisy_train_indices] = 1  # 1 = real noisy\n",
    "\n",
    "        accuracy = round((pred == y_true).mean(), 4)\n",
    "\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "        precision = round(precision_score(y_true, pred), 4)\n",
    "        recall = round(recall_score(y_true, pred), 4)\n",
    "        f1 = round(f1_score(y_true, pred), 4)\n",
    "\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1 score:\", f1)\n",
    "        print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f58732-69aa-4457-b32d-95cab6433a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, exp_num, data_frac, path_to_data):\n",
    "\n",
    "    Register(\n",
    "        dataset_name=f\"{dataset_name}_experiment{exp_num}\",\n",
    "        one_hot=False,\n",
    "        cacheable=False,\n",
    "        presplit=True\n",
    "    )(lambda: load_presplit_dataset(dataset_name, exp_num, data_frac, path_to_data))#(load_presplit_dataset(dataset_name, exp_num, data_frac, path_to_data))\n",
    "\n",
    "    fetcher = DataFetcher(f\"{dataset_name}_experiment{exp_num}\")\n",
    "    fetcher.noisy_train_indices = [i for i in noisy_indexes[f\"exp{exp_num}\"] if i < len(fetcher.x_train)]\n",
    "    curr_noisy_idxs = sum(np.array(fetcher.noisy_train_indices) < len(fetcher.x_train))\n",
    "\n",
    "    # Estimate kernel bandwidth w/ median sample pairwise distance\n",
    "    kairos = Kairos()\n",
    "    kairos.input_data(fetcher.x_train, fetcher.y_train, fetcher.x_valid, fetcher.y_valid)\n",
    "    sigma_feature = max(est_median_dist(kairos.X_valid.numpy()), est_median_dist(kairos.X_train.numpy()))\n",
    "\n",
    "    ### plot Covered vs Inspected data\n",
    "    model_name = LogisticRegression(input_dim=len(fetcher.x_train[0]), num_classes=(int(np.max(fetcher.y_train)) + 1)) #Used to be fetcher.y_train[0].size\n",
    "    exper_med = ExperimentMediator(fetcher=fetcher, pred_model=model_name, train_kwargs=train_kwargs,\n",
    "                                metric_name=metric_name, raises_error=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    list_of_data_evaluators = [\n",
    "        FixedKNNShapley(),\n",
    "        DataOob(num_models=10),\n",
    "        LavaEvaluator(random_state=42), #breaks if dataset too big\n",
    "        Kairos(sigma_feature=sigma_feature, lambda_weight=.97),\n",
    "    ]\n",
    "    eval_med = exper_med.compute_data_values(list_of_data_evaluators)\n",
    "\n",
    "    for evaluator in eval_med.data_evaluators:\n",
    "        d = get_discover_corrupted_sample_results(evaluator, fetcher)\n",
    "        eval_name = evaluator.__class__.__name__\n",
    "        plt.plot(d['axis'], d['corrupt_found'], marker=markers[eval_name], label=eval_name)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    for ax in fig.axes:\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_xlabel('')\n",
    "    fig.supylabel('% covered corrupted data')\n",
    "    fig.supxlabel('% inspected data')\n",
    "    fig.suptitle(f'Kairos: {dataset_name} ({len(fetcher.x_train)} train, {len(fetcher.x_valid)} valid, {curr_noisy_idxs} label noise) Exp 2')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    plot_valuations(\"Kairos\", eval_med, fetcher, dataset_name, exp_num)\n",
    "    plot_valuations(\"LavaEvaluator\", eval_med, fetcher, dataset_name, exp_num)\n",
    "\n",
    "    thresh_acc(\"Kairos\", eval_med, dataset_name, exp_num, fetcher)\n",
    "    thresh_acc(\"LavaEvaluator\", eval_med, dataset_name, exp_num, fetcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4ea95-f447-437e-a9a6-79a3248b7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(dataset_name=\"adult\", exp_num=2, data_frac=0.35, path_to_data=PATH_TO_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f28b96-5175-433c-9588-009f42327c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
