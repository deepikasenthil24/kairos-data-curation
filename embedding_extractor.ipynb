{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27963141",
   "metadata": {},
   "source": [
    "### Extract Embeddings\n",
    "\n",
    "Based on https://github.com/rom1504/clip-retrieval\n",
    "\n",
    "First, pip install clip-retrieval\n",
    "\n",
    "`pip install git+https://github.com/openai/CLIP.git`\n",
    "\n",
    "TODO:\n",
    "- labels fed to Kairos should be species/insect type, not \"Noisy\" vs \"clean\"\n",
    "- Need to save list of indexes of noisy images in iNat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7751b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "from utils.label_mappings import *\n",
    "\n",
    "OUT_DIR = 'data/embs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb8f4a8-bac4-49f0-9376-8f790ef30eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6ea5e9",
   "metadata": {},
   "source": [
    "#### iNaturalist Embeddings\n",
    "Need to get embeddings for all images in iNat dataset (3.3GB) for Kairos to curate the insects from the rest (noisy). 36355 rows/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1264e866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'images', 'id'],\n",
       "    num_rows: 36355\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load iNat data\n",
    "iNat36 = load_dataset(\"sxj1215/inaturalist\", split='train') #36k rows #3.3 GB\n",
    "ids = list(range(len(iNat36)))\n",
    "iNat36 = iNat36.add_column(\"id\", ids) #not idempotent\n",
    "iNat36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db7273ed-5bdb-4cf0-87d2-fa2365cfe121",
   "metadata": {},
   "outputs": [],
   "source": [
    "iNat36_label_df = pd.DataFrame({'messages': iNat36['messages'], 'id': iNat36['id']})\n",
    "\n",
    "def get_iNat_label(messages):\n",
    "    return messages[1]['content']\n",
    "    \n",
    "iNat36_label_df['species'] = iNat36_label_df['messages'].apply(get_iNat_label)\n",
    "\n",
    "def map_inat_to_clean_label(label):\n",
    "    if label in iNat_to_clean_map:\n",
    "        return iNat_to_clean_map[label]\n",
    "    else:\n",
    "        return 'noise'\n",
    "        #add index to noisy index list here?\n",
    "        \n",
    "iNat36_label_df['clean_label'] = iNat36_label_df['species'].apply(map_inat_to_clean_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe30bfb-2218-43e3-98fd-151303975624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save noisy indexes for Kairos\n",
    "noisy_idxs = iNat36_label_df[iNat36_label_df['clean_label'] == 'noise'].index\n",
    "np.save(os.path.join(OUT_DIR, f\"inat_noisy_indexes.npy\"), noisy_idxs)\n",
    "clean_idxs = iNat36_label_df[iNat36_label_df['clean_label'] != 'noise'].index\n",
    "np.save(os.path.join(OUT_DIR, f\"inat_clean_indexes.npy\"), clean_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21d054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inat_embs(inat_split_ds, file_prefix, out_dir):\n",
    "    '''\n",
    "    inat_split_ds is a split (train, test) of the iNat dataset\n",
    "    \n",
    "    file_prefix is a string to clearly label files and distinguish different groups of embeddings\n",
    "    such as 'train_inat' or 'test_inat' \n",
    "\n",
    "    out_dir is the directory the data will be saved to\n",
    "    '''\n",
    "    embs = []\n",
    "    labels = []\n",
    "    row_ids = [] # only needed if we are shuffling later\n",
    "\n",
    "    for i in tqdm(range(len(inat_split_ds))):\n",
    "        try:\n",
    "            row = inat_split_ds[i]\n",
    "            row_id = row['id']\n",
    "            img = row[\"images\"][0]\n",
    "            img = preprocess(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                feats = model.encode_image(img)\n",
    "                feats /= feats.norm(dim=-1, keepdim=True)\n",
    "            embs.append(feats.cpu().numpy()) \n",
    "            labels.append(iNat36_label_df.iloc[row_id][\"clean_label\"]) # can use species as label for now, but need clean_label later for resnet\n",
    "            row_ids.append(row_id)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            embs.append(np.zeros(512).reshape(1, 512)) #need to add placeholder so entries line up later\n",
    "            labels.append('noise')\n",
    "            row_ids.append(row_id)\n",
    "            continue\n",
    "\n",
    "    \n",
    "    print(f\"Successfully processed {len(embs)} examples\")\n",
    "\n",
    "    if embs:\n",
    "        emb_matrix = np.vstack(embs)\n",
    "        \\\n",
    "        np.save(os.path.join(OUT_DIR, f\"{file_prefix}_embeddings.npy\"), emb_matrix)\n",
    "        np.save(os.path.join(OUT_DIR, f\"{file_prefix}_labels.npy\"), labels)\n",
    "        \n",
    "        with open(os.path.join(OUT_DIR, f\"{file_prefix}_row_ids.txt\"), \"w\") as f:\n",
    "            f.write(str(row_ids))\n",
    "            \n",
    "        print(f\"Success! Saved {emb_matrix.shape} matrix to {OUT_DIR}\")\n",
    "\n",
    "\n",
    "    return embs, labels, row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ed89cf-cb49-4b0f-9150-cc7735131272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 7187/36355 [11:40<39:52, 12.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image file is truncated (122 bytes not processed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36355/36355 [58:47<00:00, 10.31it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 36355 examples\n",
      "Success! Saved (36355, 512) matrix to data/embs\n"
     ]
    }
   ],
   "source": [
    "inat_embeddings, inat_labels, inat_row_ids = generate_inat_embs(iNat36, 'inat', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b72a60-7614-4905-ad6d-61d0f9e05b1e",
   "metadata": {},
   "source": [
    "#### Kaggle clean embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce590df-ac02-44a8-ab01-6f5c5bfb0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sample_clean_data import kairos_clean_data, test_clean_data # stratefied random sampled data for kairos and rest of data for resnet test\n",
    "label_pos_in_path = 2\n",
    "\n",
    "def generate_clean_embs(images_var, file_prefix, out_dir):\n",
    "    '''\n",
    "    images var is a list of filepaths and is defined in sample_clean_data.py\n",
    "    \n",
    "    file_prefix is a string to clearly label files and distinguish different groups of embeddings\n",
    "    such as 'kairos_clean' or 'test_clean' \n",
    "\n",
    "    out_dir is the directory the data will be saved to\n",
    "    '''\n",
    "    embs = []\n",
    "    labels = []\n",
    "    filepaths = []\n",
    "    #i = 0\n",
    "\n",
    "    for path in tqdm(images_var): # path = data/clean_insect_images/Ant/Ant_283.jpg\n",
    "        # if i < 2:\n",
    "        #     print(path) # to verify same images are sampled for downstream reproducibility\n",
    "        i+=1\n",
    "        try:\n",
    "            image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                features = model.encode_image(image)\n",
    "                features /= features.norm(dim=-1, keepdim=True)    # normalize for cosine similarity\n",
    "                \n",
    "            embs.append(features.cpu().numpy())\n",
    "            labels.append(path.split('/')[label_pos_in_path])\n",
    "            filepaths.append(path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupt image {path}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(embs)} examples\")\n",
    "    \n",
    "    if embs:\n",
    "        emb_matrix = np.vstack(embs)\n",
    "        \n",
    "        np.save(os.path.join(OUT_DIR, f\"{file_prefix}_embeddings.npy\"), emb_matrix)\n",
    "        np.save(os.path.join(OUT_DIR, f\"{file_prefix}_labels.npy\"), labels)\n",
    "        \n",
    "        with open(os.path.join(OUT_DIR, f\"{file_prefix}_filepaths.txt\"), \"w\") as f:\n",
    "            f.write(str(filepaths))\n",
    "            \n",
    "        print(f\"Success! Saved {emb_matrix.shape} matrix to {OUT_DIR}\")\n",
    "\n",
    "\n",
    "    return embs, labels, filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4449eb4-d4e6-40df-b614-85986944114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/450 [00:00<00:21, 20.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/clean_insect_images/Ant/Ant_283.jpg\n",
      "data/clean_insect_images/Ant/Ant_525.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 46/450 [00:04<00:42,  9.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m kairos_clean_embeddings, kairos_clean_labels, kairos_clean_file_paths \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_clean_embs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkairos_clean_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkairos_clean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#test_clean_embeddings, test_clean_labels, test_clean_file_paths = generate_clean_embs(test_clean_data, 'test_clean', OUT_DIR)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mgenerate_clean_embs\u001b[0;34m(images_var, file_prefix, out_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     26\u001b[0m         features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/transforms/functional.py:175\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kairos_clean_embeddings, kairos_clean_labels, kairos_clean_file_paths = generate_clean_embs(kairos_clean_data, 'kairos_clean', OUT_DIR)\n",
    "test_clean_embeddings, test_clean_labels, test_clean_file_paths = generate_clean_embs(test_clean_data, 'test_clean', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff52d1-91c0-4e7e-b899-ee2ac1b0ca6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
